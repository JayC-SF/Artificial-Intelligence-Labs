{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d858d0c9",
   "metadata": {},
   "source": [
    "## Assignment 1: Bayes Classifier Question 3\n",
    "\n",
    "In this question, you will implement the **CategoricaL Naive Bayes classifier** from scratch.\n",
    "This means you must not rely on any pre-implemented models (such as those provided\n",
    "by the scikit-learn library), in order to gain a deeper understanding of how the\n",
    "algorithm works internally. \n",
    "\n",
    "Through this process, you will explore how probabilities are estimated from data, \n",
    "how predictions are made using conditional independence assumptions, and how numerical \n",
    "issues can arise in probabilistic models.\n",
    "\n",
    "Unless explicitly stated otherwise, you should not be making any changes to the library\n",
    "imports section of this notebook. In particular, you are not allowed to import any additional Python libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54ae76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3441b",
   "metadata": {},
   "source": [
    "**Data Loader and Processing Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1287df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================================\n",
    "# TODO: Using Numpy, extract CSV data. Note: Please omit header row.\n",
    "\n",
    "my_data = np.loadtxt('transactions.csv', delimiter=',', skiprows=1, dtype=str)\n",
    "\n",
    "# =====================================================================================\n",
    "# TODO: Split dataset into a feature matrix and label vector.\n",
    "\n",
    "tmp_feature_mat = my_data[:, :-1]\n",
    "tmp_label_vector = my_data[:, -1:].ravel() # make it into 1D array and flatten with ravel()\n",
    "\n",
    "# =====================================================================================\n",
    "# TODO: Convert categorical strings to integers for each feature. \n",
    "# (Example Low, Medium, High -> 0, 1, 2)\n",
    "\n",
    "feature_mat = np.zeros(tmp_feature_mat.shape, dtype=int)\n",
    "feature_mat[\"Low\" == tmp_feature_mat] = 0\n",
    "feature_mat[\"Medium\" == tmp_feature_mat] = 1\n",
    "feature_mat[\"High\" == tmp_feature_mat] = 2\n",
    "feature_mat[\"Old\" == tmp_feature_mat] = 0\n",
    "feature_mat[\"Recent\" == tmp_feature_mat] = 1\n",
    "feature_mat[\"New\" == tmp_feature_mat] = 2\n",
    "feature_mat[\"Morning\" == tmp_feature_mat] = 0\n",
    "feature_mat[\"Afternoon\" == tmp_feature_mat] = 1\n",
    "feature_mat[\"Evening\" == tmp_feature_mat] = 2\n",
    "feature_mat[\"Night\" == tmp_feature_mat] = 3\n",
    "\n",
    "label_vector = np.full(tmp_label_vector.shape, fill_value=-10, dtype=int)\n",
    "label_vector[tmp_label_vector == \"no\"] = 0\n",
    "label_vector[tmp_label_vector == \"yes\"] = 1\n",
    "\n",
    "# =====================================================================================\n",
    "# TODO: Use train_test_split function from the Scikit-learn library to split data \n",
    "# into training (75%) and test (25%) sets.\n",
    "# *IMPORTANT: Add the arguemnt random_state=42*\n",
    "# Hint: Use stratify=y to maintain class distribution\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_mat, \n",
    "    label_vector, \n",
    "    test_size=0.25, \n",
    "    train_size=0.75,\n",
    "    stratify=label_vector,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b1e22",
   "metadata": {},
   "source": [
    "**Categorical Naive Bayes Classifier**\n",
    "\n",
    "Write your code where you see the key word pass. (You should remove pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa42ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.classes = None             # Unique class labels\n",
    "        self.priors = None              # Class prior probabilities\n",
    "        self.feature_cond_probs = None  # List to store feature probabilities per class\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        - Identify unique classes\n",
    "        - Compute class priors P(y)\n",
    "        - Count occurrences of each feature value per class\n",
    "        - Apply Laplace smoothing (alpha)\n",
    "        \"\"\"\n",
    "        \n",
    "        # get the classes and their counts\n",
    "        self.classes, counts = np.unique(y, return_counts=True)\n",
    "        # sort the classes and their counts together\n",
    "        indices = np.argsort(self.classes)\n",
    "        self.classes = self.classes[indices]\n",
    "        counts = counts[indices]\n",
    "        # divide the counts of each classes by the total amount of elements.\n",
    "        # This gives the prior for each classes. P(Hi)\n",
    "        self.priors = counts/len(y)\n",
    "        \n",
    "        # find the |V| for each count and store it\n",
    "        features_category_count = np.zeros(X.shape[1], dtype=int)\n",
    "        for f_idx in range(X.shape[1]):\n",
    "            # for each category calculate |V|\n",
    "            features_category_count[f_idx] = len(np.unique(X[:,f_idx]))\n",
    "\n",
    "        # find the P(category | class) for all categories for all features\n",
    "        self.feature_cond_probs = []\n",
    "        # loop over classes\n",
    "        for c in self.classes:\n",
    "            # filter the matrix by the current category\n",
    "            X_class = X[y == c]\n",
    "            \n",
    "            # create a new class list\n",
    "            self.feature_cond_probs.append([])\n",
    "            \n",
    "            # loop over features\n",
    "            for f_idx in range(X.shape[1]):\n",
    "                categories, category_counts= np.unique(X_class[:, f_idx], return_counts=True)\n",
    "                \n",
    "                # create a new feature for the current class\n",
    "                total_words_in_c = len(X_class)\n",
    "                V = features_category_count[f_idx]\n",
    "                \n",
    "                # give them smoothed probability of 0 count by default\n",
    "                ordered_category_probs = [self.alpha/(total_words_in_c + self.alpha*V)] * V\n",
    "                        \n",
    "                # loop through all categories given the class\n",
    "                for idx, category in enumerate(categories):\n",
    "                    # get their counts\n",
    "                    count_w_c = category_counts[idx]\n",
    "                    # update their smoothed probability values since their count is greater than 0\n",
    "                    ordered_category_probs[category] = (count_w_c + self.alpha) / (total_words_in_c + self.alpha * V)\n",
    "                \n",
    "                self.feature_cond_probs[-1].append(ordered_category_probs)\n",
    "                \n",
    "\n",
    "    def compute_log_likelihood(self, X):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        - For each sample and each class:\n",
    "        - Sum log-probabilities of all features given the class\n",
    "        - Add log prior of the class\n",
    "        - Return a log-likelihood array of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        log_likelyhood = np.zeros((len(X), len(self.classes)))\n",
    "        \n",
    "        for i, x in enumerate(X):\n",
    "            # loop through the classes\n",
    "            for c in self.classes:\n",
    "                # loop through the sample's feature values\n",
    "                log_sum = np.log(self.priors[c])\n",
    "                # for each feature, retrieve the category of the sample and sum the \n",
    "                for feature_idx, category in enumerate(x):\n",
    "                    log_sum += np.log(self.feature_cond_probs[c][feature_idx][category])\n",
    "                log_likelyhood[i, c] = log_sum\n",
    "                    \n",
    "        return log_likelyhood\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        - Use compute_log_likelihood\n",
    "        - Return the class with highest log-posterior for each sample\n",
    "        \"\"\"\n",
    "        loglikelyhood = self.compute_log_likelihood(X)\n",
    "        \n",
    "        return np.argmax(loglikelyhood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd2938",
   "metadata": {},
   "source": [
    "**Classifier training and testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5644ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9000\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93        35\n",
      "           1       0.86      0.80      0.83        15\n",
      "\n",
      "    accuracy                           0.90        50\n",
      "   macro avg       0.89      0.87      0.88        50\n",
      "weighted avg       0.90      0.90      0.90        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================================\n",
    "# TODO: Create an instance of your CategoricalNaiveBayes classifier and train it\n",
    "\n",
    "cnb = CategoricalNaiveBayes()\n",
    "cnb.fit(X_train, y_train)\n",
    "\n",
    "# =====================================================================================\n",
    "# TODO: Make predictions on the test data\n",
    "\n",
    "y_pred = cnb.predict(X_test)\n",
    "\n",
    "# =====================================================================================\n",
    "# TODO: Using Scikit-learn's accuracy_score and classification_report functions, print\n",
    "# both the accuracry score and the classification_report metrics for your classifier\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\\n\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP432-Labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
